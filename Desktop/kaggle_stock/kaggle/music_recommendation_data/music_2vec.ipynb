{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "try:\n",
    "    sess = tf.Session(config=config)\n",
    "except:\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "import pickle\n",
    "transform_list = '/notebook/textData/rank_model/order_data_correct.pickle'\n",
    "\n",
    "file_data = pickle.load(open(transform_list,'r'))\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "len(file_data)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "prod_seq = []\n",
    "name_seq = []\n",
    "uuid_seq = []\n",
    "max_len = 0\n",
    "max_seq = []\n",
    "max_name = []\n",
    "uuid_max = []\n",
    "data_seq = []\n",
    "name_dict = {}\n",
    "not_see_count = 0\n",
    "count = 0\n",
    "for index in range(len(file_data)):\n",
    "    user_order = file_data[index]\n",
    "    for index_prod in range(len(user_order)):\n",
    "        count += 1\n",
    "        one_order = user_order[index_prod]\n",
    "        id_prod = []\n",
    "        name_prod = []\n",
    "        pre_order = \"\"\n",
    "        for index_order in range(len(one_order)):\n",
    "            curr_order = one_order[index_order]\n",
    "            curr_id = curr_order.split(\"|\")[-2] # get the id\n",
    "            if curr_id ==pre_order:\n",
    "                continue\n",
    "            else:\n",
    "                id_prod.append(curr_id)\n",
    "                name_prod.append(curr_order.split(\"|\")[-3])  # get the name\n",
    "                pre_order = curr_id\n",
    "            \n",
    "            if curr_id not in name_dict:\n",
    "                name_dict[curr_id] = curr_order.split(\"|\")[-3]  # get the sku id \n",
    "        # get the not see count\n",
    "        order_get = id_prod[-1]\n",
    "        if order_get not in id_prod[:-1]:\n",
    "            not_see_count += 1\n",
    "        \n",
    "            \n",
    "        if len(id_prod)>=2:\n",
    "            \n",
    "            if len(id_prod)>max_len:\n",
    "                \n",
    "                max_seq.append(id_prod)\n",
    "                max_name.append(name_prod)\n",
    "                max_len = max(len(id_prod), max_len)\n",
    "                uuid_max.append(curr_order.split(\"|\")[0])\n",
    "                print len(id_prod)\n",
    "\n",
    "            prod_seq.append(id_prod)\n",
    "            name_seq.append(name_prod)\n",
    "            uuid_seq.append(curr_order.split(\"|\")[0])\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "curr_order\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "len(uuid_seq)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "len(prod_seq)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "prod_seq[240]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "len(prod_seq)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "# Build dictionary of words\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    # Turn sentences (list of strings) into lists of words\n",
    "    #split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in sentences for x in sublist]\n",
    "    \n",
    "    # Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = []\n",
    "    \n",
    "    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    \n",
    "    # Now create the dictionary\n",
    "    word_dict = {}\n",
    "    # For each word, that we want in the dictionary, add it, then make it\n",
    "    # the value of the prior dictionary length\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return(word_dict)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "sentences = prod_seq\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "len(sentences)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "import collections\n",
    "\n",
    "vocabulary_size = min(50000, len(name_dict) )\n",
    "\n",
    "word_dictionary = build_dictionary(prod_seq, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "word_dictionary_rev\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "len(name_dict)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "len(word_dictionary_rev)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "list_product = []\n",
    "for key in word_dictionary_rev.keys():\n",
    "    value = word_dictionary_rev[key]\n",
    "    if value in name_dict:\n",
    "        list_product.append(name_dict[value])\n",
    "    else:\n",
    "        list_product.append(\"unk\")\n",
    "    \n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "len(list_product)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "# Turn text data into lists of integers from dictionary\n",
    "def text_to_numbers(sentences, word_dict):\n",
    "    # Initialize the returned data\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        # For each word, either use selected index or rare word index\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "prod_seq[301]\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "text_data = text_to_numbers(prod_seq, word_dictionary)\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "len(word_dictionary)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "valid_words = prod_seq[30]\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "len(word_dictionary)\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "valid_examples[1]\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "len(word_dictionary)\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "name_dict[word_dictionary_rev[valid_examples[0]]]\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "# Get validation word keys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "from nltk.corpus import stopwords\n",
    "from six.moves import urllib\n",
    "\n",
    "import urllib\n",
    "\n",
    "# Start a graph session\n",
    "\n",
    "\n",
    "# Declare model parameters\n",
    "batch_size = 50\n",
    "embedding_size = 200\n",
    "generations = 10000\n",
    "print_loss_every = 500\n",
    "\n",
    "\n",
    "num_sampled = int(batch_size/2)    # Number of negative examples to sample.\n",
    "window_size = 4       # How many words to consider left and right.\n",
    "\n",
    "# Declare stop words\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# We pick five test words. We are expecting synonyms to appear\n",
    "print_valid_every = 2000\n",
    "\n",
    "# Generate data randomly (N words behind, target, N words ahead)\n",
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentence to start\n",
    "        rand_sentence = np.random.choice(sentences)\n",
    "        # Generate consecutive windows to look at\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "            \n",
    "        # extract batch and labels\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]).astype(int))\n",
    "    \n",
    "    return(batch_data, label_data)\n",
    "    \n",
    "\n",
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([len(word_dictionary), embedding_size], -1.0, 1.0), name = \"product_vec\")\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding:\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)\n",
    "\n",
    "# Get loss from prediction\n",
    "loss_get = tf.nn.nce_loss(weights=nce_weights,            # Tensor of shape(50000, 128)\n",
    "                          biases=nce_biases,              # vector of zeros; len(128)\n",
    "                          labels=y_target,            # labels == context words enums\n",
    "                          inputs=embed,                   # Tensor of shape(128, 128)\n",
    "                          num_sampled=num_sampled,        # 64: randomly chosen negative (rare) words\n",
    "                          num_classes=vocabulary_size)   # 50000: by construction\n",
    "loss = tf.reduce_mean(loss_get)\n",
    "\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "LOG_DIR = '/tmp/testing/product2vce'\n",
    "#tensorboard --logdir=/tmp/testing/example_2_2\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR, graph=tf.get_default_graph())\n",
    "\n",
    "# Run the skip gram model.\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    _, summary = sess.run([optimizer, merged_summary_op], feed_dict=feed_dict)\n",
    "    \n",
    "    summary_writer.add_summary(summary, i)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = name_dict[word_dictionary_rev[valid_examples[0]]] #word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            #name_dict[word_dictionary_rev[valid_examples[0]]]\n",
    "            log_str = \"Nearest to {}::\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = name_dict[word_dictionary_rev[valid_examples[k]]] #word_dictionary_rev[nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)\n",
    "            print \"#################\"\n",
    "            \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "import pandas as pd\n",
    "new_df = pd.DataFrame() #creates a new dataframe that's empty\n",
    "new_df[\"word\"] = list_product\n",
    "import csv\n",
    "new_df.to_csv(os.path.join(LOG_DIR, 'output2.tsv'), sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "# Create randomly initialized embedding weights which will be trained.\n",
    "\n",
    "# Format: tensorflow/tensorboard/plugins/projector/projector_config.proto\n",
    "config = projector.ProjectorConfig()\n",
    "\n",
    "# You can add multiple embeddings. Here we add only one.\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = 'product_vec'\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "embedding.metadata_path = os.path.join(LOG_DIR, 'output2.tsv')\n",
    "\n",
    "# Use the same LOG_DIR where you stored your checkpoint.\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "# read this file during startup.\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), 1000)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "normalized_embeddings\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "normalized_embeddings_get = sess.run(normalized_embeddings)\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "normalized_embeddings_get.shape\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "import pickle\n",
    "\n",
    "prod2Vec_save = '/notebook/textData/rank_model/product_vec_update_10_July.pickle'\n",
    "\n",
    "with open(prod2Vec_save, 'wb') as handle:\n",
    "    pickle.dump(normalized_embeddings_get, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "normalized_embeddings_get.shape\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "res = word_dictionary_rev.get(5240)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "res\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "len(word_dictionary_rev)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "res\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "prod2Vec_mapping_sku = '/notebook/textData/rank_model/sku_index_update_10_July.pickle'\n",
    "\n",
    "with open(prod2Vec_mapping_sku, 'wb') as handle:\n",
    "    pickle.dump(word_dictionary_rev, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# # predict the next product, which is based on the shoping history\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "len(text_data)\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "len(word_dictionary_rev.keys())\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[255]:\n",
    "\n",
    "train_x = []\n",
    "label = []\n",
    "for item in text_data:\n",
    "    train_x.append(item[:-1])\n",
    "    label.append(item[-1])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[270]:\n",
    "\n",
    "hidden_information = 100\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "seq_max_len = 20 # Sequence max length\n",
    "\n",
    "x_data = tf.placeholder(tf.int32, [ None, seq_max_len], name = \"sequence_prod\")\n",
    "y_data = tf.placeholder(tf.int32, [ None, 1], name = \"target\")\n",
    "\n",
    "seqlen = tf.placeholder(tf.int32, [None], name = \"sequence_length\")\n",
    "\n",
    "\n",
    "\n",
    "y_target = tf.nn.embedding_lookup(normalized_embeddings_get, y_data)\n",
    "\n",
    "embedding_output = tf.nn.embedding_lookup(normalized_embeddings_get, x_data)\n",
    "\n",
    "\n",
    "# In[271]:\n",
    "\n",
    "# cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "# rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "# tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "# output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state\n",
    "\n",
    "\n",
    "# In[343]:\n",
    "\n",
    "with tf.variable_scope('cell_def_2'):\n",
    "    cellTwo= tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(hidden_information, forget_bias=0.1,\n",
    "                                                        activation = tf.nn.relu,\n",
    "                                                        state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "try:\n",
    "    with tf.variable_scope(\"lstm_chainThree\"):\n",
    "        output_rnn,final_states=tf.nn.dynamic_rnn(cellTwo, embedding_output, sequence_length = seqlen, dtype=tf.float32)\n",
    "\n",
    "        #output_rnn,final_states=tf.nn.dynamic_rnn(cellTwo, embedding_output, sequence_length = seqlen, dtype=tf.float32)\n",
    "        output = tf.nn.dropout(output_rnn, dropout_keep_prob)\n",
    "\n",
    "        # Get output of RNN sequence,     # and change back dimension to [batch_size, n_step, n_input]\n",
    "        output = tf.transpose(output, [1, 0, 2])\n",
    "        # add attension here and categaorial featuers\n",
    "        last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        # concate the categories features    \n",
    "    \n",
    "except:\n",
    "    with tf.variable_scope(\"lstm_chainThree\", reuse = True):\n",
    "        output_rnn,final_states=tf.nn.dynamic_rnn(cellTwo, embedding_output,  dtype=tf.float32)\n",
    "        #output_rnn,final_states=tf.nn.dynamic_rnn(cellTwo, embedding_output, sequence_length = seqlen, dtype=tf.float32)\n",
    "        output = tf.nn.dropout(output_rnn, dropout_keep_prob)\n",
    "\n",
    "        # Get output of RNN sequence,     # and change back dimension to [batch_size, n_step, n_input]\n",
    "        output = tf.transpose(output, [1, 0, 2])\n",
    "        # add attension here and categaorial featuers\n",
    "        last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        # concate the categories features    \n",
    "\n",
    "\n",
    "# In[344]:\n",
    "\n",
    "A = tf.Variable(tf.random_normal(shape=[100,200]))\n",
    "b = tf.Variable(tf.random_normal(shape=[200]))\n",
    "model_output = tf.add(tf.matmul(last, A), b)\n",
    "\n",
    "\n",
    "# In[345]:\n",
    "\n",
    "# Declare model operations\n",
    "\n",
    "\n",
    "# In[346]:\n",
    "\n",
    "# Declare loss function (L2 loss)\n",
    "loss = tf.reduce_mean(tf.square(y_target - model_output))\n",
    "\n",
    "\n",
    "# In[347]:\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.005)\n",
    "optimizer_train = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# # training the network\n",
    "\n",
    "# In[277]:\n",
    "\n",
    "len(batch_seq)\n",
    "\n",
    "\n",
    "# In[280]:\n",
    "\n",
    "np.array(batch_seq)\n",
    "\n",
    "\n",
    "# In[289]:\n",
    "\n",
    "batch_x = np.array([np.array(x) for x in train_x[epoch*batch:(epoch+1)*batch]])\n",
    "\n",
    "\n",
    "# In[293]:\n",
    "\n",
    "batch_y = np.array(label[epoch*batch:(epoch+1)*batch])\n",
    "\n",
    "\n",
    "# In[295]:\n",
    "\n",
    "batch_y.shape\n",
    "\n",
    "\n",
    "# In[297]:\n",
    "\n",
    "seqlen\n",
    "\n",
    "\n",
    "# In[298]:\n",
    "\n",
    "batch_seq\n",
    "\n",
    "\n",
    "# In[299]:\n",
    "\n",
    "batch = 50\n",
    "num_epoch = len(text_data)/batch -1\n",
    "epoch = 0 \n",
    "display_step = 3\n",
    "while epoch<num_epoch:\n",
    "    batch_x = np.array([np.array(x) for x in train_x[epoch*batch:(epoch+1)*batch]])\n",
    "    batch_seq = np.array([len(x) for x in batch_x])\n",
    "    batch_y = np.array(label[epoch*batch:(epoch+1)*batch])\n",
    "\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "        # Run optimization op (backprop)\n",
    "        \n",
    "        \n",
    "    _, c, = sess.run([optimizer_train , loss], feed_dict={x_data: batch_x, y_data: batch_y,\n",
    "                                   seqlen: batch_seq})\n",
    "    if step % display_step == 0:\n",
    "        print \"the loss is %f\" %c\n",
    "\n",
    "\n",
    "    step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[234]:\n",
    "\n",
    "batch_x = text_data[epoch*batch:(epoch+1)*batch]\n",
    "\n",
    "\n",
    "# In[236]:\n",
    "\n",
    "len(batch_x)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[231]:\n",
    "\n",
    "# train_x = []\n",
    "\n",
    "\n",
    "# # padding to zero\n",
    "\n",
    "# In[301]:\n",
    "\n",
    "batch_x = train_x[epoch*batch:(epoch+1)*batch]\n",
    "\n",
    "\n",
    "# In[302]:\n",
    "\n",
    "train_x_array = np.zeros((len(text_data), cut_len))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[303]:\n",
    "\n",
    "train_x_array.shape\n",
    "\n",
    "\n",
    "# In[318]:\n",
    "\n",
    "len_get = []\n",
    "count_25 = 0\n",
    "count_24 = 0\n",
    "train_x_get = []\n",
    "label_get = []\n",
    "cut_len = 20\n",
    "\n",
    "train_x_array = np.zeros((len(text_data), cut_len))\n",
    "label_y_array = np.zeros((len(text_data)))\n",
    "for index in range(len(text_data)):\n",
    "    item = text_data[index]\n",
    "    len_get.append(len(item))\n",
    "    label_mask = item[-1]\n",
    "\n",
    "    if len(item)>cut_len:\n",
    "        new_item = item[:cut_len]\n",
    "        train_x_get.append(item[:cut_len])\n",
    "        label_get.append(item[cut_len])\n",
    "        count_25 += 1\n",
    "    else:\n",
    "        label_get.append(item[-1])\n",
    "        new_item = item[:-1]\n",
    "        for i in range(cut_len):\n",
    "            new_item.append(0)\n",
    "            if len(new_item) == cut_len:\n",
    "                train_x_get.append(new_item)\n",
    "                break\n",
    "        count_24 += 1\n",
    "    train_x_array[index, :] = new_item\n",
    "    label_y_array[index] = label_mask\n",
    "        #print new_item\n",
    "\n",
    "\n",
    "# In[319]:\n",
    "\n",
    "train_x_array.shape\n",
    "\n",
    "\n",
    "# In[320]:\n",
    "\n",
    "label_y_array.shape\n",
    "\n",
    "\n",
    "# In[324]:\n",
    "\n",
    "batch_x = train_x_array[epoch*batch:(epoch+1)*batch]\n",
    "\n",
    "\n",
    "# In[326]:\n",
    "\n",
    "batch_x.shape\n",
    "\n",
    "\n",
    "# In[330]:\n",
    "\n",
    "batch_x.shape\n",
    "\n",
    "\n",
    "# In[331]:\n",
    "\n",
    "y_data\n",
    "\n",
    "\n",
    "# In[360]:\n",
    "\n",
    "aa = np.reshape(batch_y, (50, 1))\n",
    "\n",
    "\n",
    "# In[355]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[356]:\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "# In[357]:\n",
    "\n",
    "num_epoch = len(text_data)/batch -1\n",
    "\n",
    "\n",
    "# In[361]:\n",
    "\n",
    "label_y_array\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "batch = 50\n",
    "num_epoch = 200#len(text_data)/batch -1\n",
    "epoch = 0 \n",
    "display_step = 10\n",
    "while epoch<num_epoch:\n",
    "    epoch = random.randrange(1, num_epoch, 1)\n",
    "    try:\n",
    "        batch_x = train_x_array[epoch*batch:(epoch+1)*batch]\n",
    "        batch_y = np.reshape(label_y_array[epoch*batch:(epoch+1)*batch], (50, 1))\n",
    "        step = 1\n",
    "        # Keep training until reach max iterations\n",
    "            # Run optimization op (backprop)\n",
    "        _, c, = sess.run([optimizer_train , loss], feed_dict={x_data: batch_x, y_data: batch_y, dropout_keep_prob:0.9})\n",
    "                                       #seqlen: batch_seq})\n",
    "        print c\n",
    "        step += 1\n",
    "    except:\n",
    "        continue\n",
    "    #print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "# In[314]:\n",
    "\n",
    "batch = 100\n",
    "num_epoch = len(train_x)/batch -1\n",
    "epoch = 0 \n",
    "# while epoch<10:\n",
    "batch_x = train_x_array[epoch*batch:(epoch+1)*batch]\n",
    "batch_y = label_y_array[epoch*batch:(epoch+1)*batch]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[316]:\n",
    "\n",
    "batch_x.shape\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "num_epoch\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Training loop\n",
    "loss_vec = []\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(len(x_vals), size= batch_size)\n",
    "    rand_x = x_vals[rand_index]\n",
    "    rand_y = np.reshape(y_vals[rand_index],(batch_size,1))\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    if (i+1)%25==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n",
    "        print('Loss = ' + str(temp_loss))\n",
    "\n",
    "\n",
    "# In[114]:\n",
    "\n",
    "y_target\n",
    "\n",
    "\n",
    "# In[115]:\n",
    "\n",
    "hidden_information\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Neural net parameters\n",
    "\n",
    "learning_rate = 1e-3\n",
    "embedding_size = hidden_information\n",
    "n_labels = 200 #len(np.unique(Y))\n",
    "n_hidden = 300\n",
    "n_class = 200\n",
    "b = tf.Variable(tf.random_normal(shape=[n_class]))\n",
    "\n",
    "\n",
    "# Neural network weights\n",
    "try:\n",
    "    with tf.variable_scope(\"weight__get_4\"):\n",
    "        h = tf.get_variable(name='h2', shape=[embedding_size, n_hidden],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W_out = tf.get_variable(name='out_w', shape=[n_hidden, n_class],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "except:\n",
    "    with tf.variable_scope(\"weight__get_4\", reuse = True):\n",
    "        h = tf.get_variable(name='h2', shape=[embedding_size, n_hidden],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W_out = tf.get_variable(name='out_w', shape=[n_hidden, n_class],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import cross_validation\n",
    "import numpy as np\n",
    "\n",
    "boston = learn.datasets.load_dataset('boston')\n",
    "x, y = boston.data, boston.target\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(\n",
    "x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "total_len = X_train.shape[0]\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "batch_size = 10\n",
    "display_step = 1\n",
    "dropout_rate = 0.9\n",
    "# Network Parameters\n",
    "n_hidden_1 = 32 # 1st layer number of features\n",
    "n_hidden_2 = 200 # 2nd layer number of features\n",
    "n_hidden_3 = 200\n",
    "n_hidden_4 = 256\n",
    "n_input = X_train.shape[1]\n",
    "n_classes = 1\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 13])\n",
    "y = tf.placeholder(\"float\", [None])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], 0, 0.1)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], 0, 0.1)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], 0, 0.1)),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4], 0, 0.1)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes], 0, 0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], 0, 0.1)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2], 0, 0.1)),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3], 0, 0.1)),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4], 0, 0.1)),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], 0, 0.1))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.square(pred-y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(total_len/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch-1):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = Y_train[i*batch_size:(i+1)*batch_size]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c, p = sess.run([optimizer, cost, pred], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        # sample prediction\n",
    "        label_value = batch_y\n",
    "        estimate = p\n",
    "        err = label_value-estimate\n",
    "        print (\"num batch:\", total_batch)\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\",                 \"{:.9f}\".format(avg_cost))\n",
    "            print (\"[*]----------------------------\")\n",
    "            for i in xrange(3):\n",
    "                print (\"label value:\", label_value[i],                     \"estimated value:\", estimate[i])\n",
    "            print (\"[*]============================\")\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "#     # Test model\n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#     print (\"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "x_data\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "last\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # For the parameters\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "seq_max_len = 20\n",
    "n_hidden = 64\n",
    "\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, 1])\n",
    "y = tf.placeholder(\"float\", [None, None])\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, 2]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([2]))\n",
    "}\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "hidden_information = 100\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "\n",
    "x_data = tf.placeholder(tf.int32, [ None, 25], name = \"search_term\")\n",
    "\n",
    "embedding_output = tf.nn.embedding_lookup(normalized_embeddings_get, x_data)\n",
    "\n",
    "\n",
    "with tf.variable_scope('cell_def_2'):\n",
    "    cellTwo= tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(hidden_information, forget_bias=0.1,\n",
    "                                                        activation = tf.nn.relu,\n",
    "                                                        state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "\n",
    "try:\n",
    "    with tf.variable_scope(\"lstm_chainThree\"):\n",
    "        output_rnn,final_states=tf.nn.dynamic_rnn(cellTwo, embedding_output, dtype=tf.float32)\n",
    "        output = tf.nn.dropout(output_rnn, dropout_keep_prob)\n",
    "\n",
    "        # Get output of RNN sequence,     # and change back dimension to [batch_size, n_step, n_input]\n",
    "        output = tf.transpose(output, [1, 0, 2])\n",
    "        # add attension here and categaorial featuers\n",
    "        last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        # concate the categories features    \n",
    "    \n",
    "except:\n",
    "    with tf.variable_scope(\"lstm_chainThree\", reuse = True):\n",
    "        output_rnn,final_states=tf.nn.dynamic_rnn(cellTwo, embedding_output, dtype=tf.float32)\n",
    "        output = tf.nn.dropout(output_rnn, dropout_keep_prob)\n",
    "\n",
    "        # Get output of RNN sequence,     # and change back dimension to [batch_size, n_step, n_input]\n",
    "        output = tf.transpose(output, [1, 0, 2])\n",
    "        # add attension here and categaorial featuers\n",
    "        last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        # concate the categories features    \n",
    "\n",
    "\n",
    "# In[73]:\n",
    "\n",
    "last\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "# Neural net parameters\n",
    "\n",
    "learning_rate = 1e-3\n",
    "embedding_size = hidden_information\n",
    "#n_labels = len(np.unique(Y))\n",
    "n_hidden = 60\n",
    "n_class = 2\n",
    "b = tf.Variable(tf.random_normal(shape=[2]))\n",
    "\n",
    "\n",
    "# Neural network weights\n",
    "try:\n",
    "    with tf.variable_scope(\"weight__get_4\"):\n",
    "        h = tf.get_variable(name='h2', shape=[embedding_size, n_hidden],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W_out = tf.get_variable(name='out_w', shape=[n_hidden, n_class],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "except:\n",
    "    with tf.variable_scope(\"weight__get_4\", reuse = True):\n",
    "        h = tf.get_variable(name='h2', shape=[embedding_size, n_hidden],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W_out = tf.get_variable(name='out_w', shape=[n_hidden, n_class],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "layer_1 = tf.matmul(combine_tensor,h)\n",
    "\n",
    "layer_activate = tf.nn.relu(layer_1)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # image relevance analysis\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # relevance analysis \n",
    "\n",
    "# In[35]:\n",
    "\n",
    "import pickle\n",
    "transform_list = '/notebook/textData/rank_model/nearest.pickle'\n",
    "\n",
    "nearest_data_prod2vec = pickle.load(open(transform_list,'r'))\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "import os\n",
    "\n",
    "directory = '/notebook/textData/rank_model/image_get/imag_get'\n",
    "data_get = []\n",
    "import glob\n",
    "dict_get = {}\n",
    "for filename in glob.glob(os.path.join(directory, '*.jpg')):\n",
    "    dict_get[filename.split(\"/\")[-1][:-4]] = filename\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "len(nearest_data)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import random \n",
    "data_index = random.sample(range(len(nearest_data) -1), 30)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "nearest_data[data_index[1]]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "data_index\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "list_data\n",
    "\n",
    "\n",
    "# # get the name and price and sku weights\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "data_path = '/notebook/textData/rank_model/disney_products.json'\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "sku_name = {}\n",
    "sku_price = {}\n",
    "with open(data_path) as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        data.append(item)\n",
    "        if \"name\" in item.keys():\n",
    "            sku_name[item[\"sku\"]] = item[\"name\"]\n",
    "        elif \"<PRODUCTNAME\" in item.keys():\n",
    "            sku_name[item[\"sku\"]] = item[\"<PRODUCTNAME\"]\n",
    "        else:\n",
    "            print (item)\n",
    "        if \"final_price\" in item.keys():\n",
    "            sku_price[item[\"sku\"]] = item[\"final_price\"]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "len(sku_name)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "transform_list = '/notebook/textData/rank_model/sku_wight.csv'\n",
    "\n",
    "sku_weight = pd.read_csv(transform_list)\n",
    "sku_weights_valid = sku_weight[['sku','weight']]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "item[\"price\"]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "item[\"sku_fallback\"]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "sku_weights_valid.columns\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "sku_weights_dict = dict(zip(sku_weights_valid['sku'], sku_weights_valid[\"weight\"]))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "len(dict_get)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "print ('the next view sequence')\n",
    "\n",
    "def plot_list(sku_sequence, sku_name, sku_price,sku_weights_valid):\n",
    "    for flat_list in sku_sequence:\n",
    "        print (\"#########\")\n",
    "        print (flat_list)\n",
    "        for key in flat_list:\n",
    "            try:                \n",
    "                print (\"the product name is\")\n",
    "                print (sku_name[key])\n",
    "                print (\"the price is %f\" % sku_price[key])\n",
    "#                 if key in sku_weights_dict.keys():\n",
    "#                     sku_weights_valid[key]\n",
    "#                     print (\"the weight is %f\" % sku_weights_valid[key])\n",
    "                print \"show the image\"\n",
    "\n",
    "                plt.figure()\n",
    "                img=mpimg.imread(dict_get[key])\n",
    "                imgplot = plt.imshow(img)\n",
    "                plt.show()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "# # product2vec nearly neighbors\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import pickle\n",
    "transform_list = '/notebook/textData/rank_model/nearest_update_one.pickle'\n",
    "\n",
    "nearest_data_image = pickle.load(open(transform_list,'r'))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "nearest_data_image\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "len(nearest_data_image)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "data_index = random.sample(range(1000), 30)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def get_NN(data_index, nearest_data_prod2vec):\n",
    "    list_data = []\n",
    "    for item_query in data_index: \n",
    "        list_one = nearest_data_prod2vec[item_query]\n",
    "\n",
    "        flat_list = [list_one[0]]\n",
    "        for item in list_one[1]:\n",
    "            flat_list.append(item)\n",
    "        list_data.append(flat_list)\n",
    "    return list_data\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "list_data = get_NN(data_index, nearest_data_prod2vec)\n",
    "list_image = get_NN(data_index, nearest_data_image)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "nearest_data_prod2vec\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "prod2vec = {}\n",
    "for item in nearest_data_prod2vec:\n",
    "    prod2vec[item[0]] = item[1]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "image2vec = {}\n",
    "\n",
    "for item in nearest_data_image:\n",
    "    image2vec[item[0]] = item[1]\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "result_get = set(image2vec).intersection(prod2vec)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "list_data = []\n",
    "list_data_image = []\n",
    "for item in result_get:\n",
    "    list_data.append([item] + prod2vec[item])\n",
    "    list_data_image.append([item, item] + image2vec[item])\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "list_data_image\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "list_data\n",
    "\n",
    "\n",
    "# # comparing the index part\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plot_list(list_data, sku_name, sku_price,sku_weights_valid)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plot_list(list_data_image, sku_name, sku_price,sku_weights_valid)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#close_word = name_dict[word_dictionary_rev[valid_examples[k]]] #word_dictionary_rev[nearest[k]]\n",
    "\n",
    "\n",
    "# # show the images\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Step 6: Visualize the embeddings.\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "plot_only = 20\n",
    "valid_examples = range(plot_only)\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=35, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "  low_dim_embs = tsne.fit_transform(normalized_embeddings_get[:plot_only, :])\n",
    "  labels = [unicode(name_dict[word_dictionary_rev[valid_examples[i]]], \"latin-1\") for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels)\n",
    "except ImportError:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "valid_examples\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "close_word = name_dict[word_dictionary_rev[valid_examples[i]]] #word_dictionary_rev[nearest[k]]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "tf.__version__\n",
    "\n",
    "PATH = \"/notebook/textData/rank_model/\"\n",
    "\n",
    "LOG_DIR = PATH + 'embedding-logs_13'\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "prod2Vec_save = '/notebook/textData/rank_model/product_vec_update_10_July.pickle'\n",
    "prod2Vec_mapping_sku = '/notebook/textData/rank_model/sku_index_update_10_July.pickle'\n",
    "\n",
    "\n",
    "index_sku = pickle.load(open(prod2Vec_mapping_sku,'r'))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "len(index_sku)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "import os\n",
    "\n",
    "directory = '/notebook/textData/rank_model/image_get/imag_get'\n",
    "data_get = []\n",
    "import glob\n",
    "dict_get = {}\n",
    "for filename in glob.glob(os.path.join(directory, '*.jpg')):\n",
    "    dict_get[filename.split(\"/\")[-1][:-4]] = filename\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "id_get = index_sku.values()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "number_items = len(id_get)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img_data=[]\n",
    "count = 0\n",
    "for key in id_get:\n",
    "    try:\n",
    "        #plt.figure()\n",
    "        img=mpimg.imread(dict_get[key])\n",
    "        input_img_resize=cv2.resize(img,(100, 100))\n",
    "        img_data.append(input_img_resize)\n",
    "        \n",
    "        if count %1000 ==0:\n",
    "            print (count)\n",
    "\n",
    "        #imgplot = plt.imshow(input_img_resize)\n",
    "        #plt.show()\n",
    "    except:\n",
    "        continue\n",
    "    count += 1\n",
    "    \n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "img_data = np.array(img_data)\n",
    "imgplot = plt.imshow(img_data[144])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "import pickle\n",
    "prod2Vec_save_path = '/notebook/textData/rank_model/product_vec_update_10_July.pickle'\n",
    "prod2Vec_save = pickle.load(open(prod2Vec_save_path,'r'))\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "feature_vectors = np.array(prod2Vec_save).reshape((number_items, 200))\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "feature_vectors.shape\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "data_path = '/notebook/textData/rank_model/disney_products.json'\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "sku_name = {}\n",
    "sku_price = {}\n",
    "with open(data_path) as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        data.append(item)\n",
    "        if \"name\" in item.keys():\n",
    "            sku_name[item[\"sku\"]] = item[\"name\"]\n",
    "        elif \"<PRODUCTNAME\" in item.keys():\n",
    "            sku_name[item[\"sku\"]] = item[\"<PRODUCTNAME\"]\n",
    "        else:\n",
    "            print (item)\n",
    "        if \"final_price\" in item.keys():\n",
    "            sku_price[item[\"sku\"]] = item[\"final_price\"]\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "name_list = []\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "for key in id_get:\n",
    "    name_list.append(sku_name.get(key, \"None\").encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "num_of_samples=feature_vectors.shape[0]\n",
    "num_of_samples_each_class = 10\n",
    "\n",
    "features = tf.Variable(feature_vectors, name='features')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(LOG_DIR)\n",
    "except:\n",
    "    pass\n",
    "import pandas as pd\n",
    "new_df = pd.DataFrame() #creates a new dataframe that's empty\n",
    "new_df[\"word\"] = name_list\n",
    "import csv\n",
    "new_df.to_csv(os.path.join(LOG_DIR, 'metadata_6_classes.tsv'), sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "# Taken from: https://github.com/tensorflow/tensorflow/issues/6322\n",
    "def images_to_sprite(data):\n",
    "    \"\"\"Creates the sprite image along with any necessary padding\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    # Inverting the colors seems to look better for MNIST\n",
    "    #data = 1 - data\n",
    "\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "sprite = images_to_sprite(img_data)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "cv2.imwrite(os.path.join(LOG_DIR, 'sprite_4_classes.png'), sprite)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "try:\n",
    "    sess = tf.Session(config=config)\n",
    "except:\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "saver = tf.train.Saver([features])\n",
    "\n",
    "sess.run(features.initializer)\n",
    "saver.save(sess, os.path.join(LOG_DIR, 'images_5_classes.ckpt'))\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "# One can add multiple embeddings.\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = features.name\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "embedding.metadata_path = os.path.join(LOG_DIR, 'metadata_6_classes.tsv')\n",
    "# Comment out if you don't want sprites\n",
    "embedding.sprite.image_path = os.path.join(LOG_DIR, 'sprite_4_classes.png')\n",
    "embedding.sprite.single_image_dim.extend([img_data.shape[1], img_data.shape[1]])\n",
    "# Saves a config file that TensorBoard will read during startup.\n",
    "projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "LOG_DIR\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
